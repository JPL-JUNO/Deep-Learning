\chapter{word2vec\label{Ch03}}
在 \nameref{Ch02} 中，我们使用基于计数的方法得到了单词的分布式表示。本章我们将讨论该方法的替代方法，即基于推理的方法。

\section{基于推理的方法和神经网络}
基于计数的方法使用整个语料库的统计数据（共现矩阵和 PPMI 等），通过一次处理（SVD 等）获得单词的分布式表示。而基于推理的方法使用神经网络，通常在 mini-batch 数据上进行学习。这意味着神经网络一次只需要看一部分学习数据（mini-batch），并反复更新权重。
\subsection{基于推理的方法的概要}
\figures{fig3-2}{基于两边的单词（上下文），预测“？”处出现什么单词}
解开 \autoref{fig3-2} 中的推理问题并学习规律，就是基于推理的方法的主要任
务。通过反复求解这些推理问题，可以学习到单词的出现模式。从“模型视
角”出发，这个推理问题如 \autoref{fig3-3} 所示。

\begin{tcolorbox}
    基于推理的方法和基于计数的方法一样，也基于分布式假设。分布式假设假设“单词含义由其周围的单词构成”。基于推理的方法将这一假设归结为了上面的预测问题。由此可见，不管是哪种方法，如何对基于分布式假设的“单词共现”建模都是最重要的研究主题。
\end{tcolorbox}
\figures{fig3-3}{基于推理的方法：输入上下文，模型输出各个单词的出现概率}
\subsection{神经网络中单词的处理方法}
神经网络无法直接处理 you 或 say 这样的单词，要用神经网络处理单词，需要先将单词转化为固定长度的向量。对此，一种方式是将单词转换为 one-hot 表示（one-hot 向量）。在 one-hot 表示中，只有一个元素是 1，其他元素都是 0。像这样，只要将单词转化为固定长度的向量，神经网络的输入层的神经元个数就可以固定下来。
\section{简单的 word2vec}
\begin{tcolorbox}
    word2vec 一词最初用来指程序或者工具，但是随着该词的流行，在某些语境下，也指神经网络的模型。正确地说，CBOW 模型和 skip-gram 模型是 word2vec 中使用的两个神经网络。
\end{tcolorbox}

\subsection{CBOW模型的推理}
CBOW 模型的输入是上下文。这个上下文用 \verb|['you', 'goodbye']| 这样的单词列表表示。我们将其转换为 one-hot 表示，以便 CBOW 模型可以进行处理。在此基础上，CBOW 模型的网络可以画成 \autoref{fig3-9} 这样。

\autoref{fig3-9} 是 CBOW 模型的网络。它有两个输入层（这里，因为我们对上下文仅考虑两个单词，所以输入层有两个。如果对上下文考虑 $N$ 个单词，则输入层会有 $N$ 个。），经过中间层到达输出层。这里，从输入层到中间层的变换由相同的全连接层（权重为 $\bm{W}_{in}$）完成，从中间层到输出层神经元的变换由另一个全连接层（权重为 $\bm{W}_{out}$）完成。

\figures{fig3-9}{CBOW 模型的网络结构}

中间层的神经元数量比输入层少这一点很重要。中间层需要将预测单词所需的信息压缩保存，从而产生密集的向量表示。这时，中间层被写入了我们人类无法解读的代码，这相当于“编码”工作。而从中间层的信息获得期望结果的过程则称为“解码”。这一过程将被编码的信息复原为我们可以理解的形式。

\subsection{CBOW 模型的学习}
CBOW 模型只是学习语料库中单词的出现模式。如果语料库不一样，学习到的单词的分布式表示也不一样。比如，只使用“体育”相关的文章得到的单词的分布式表示，和只使用“音乐”相关的文章得到的单词的分布式表示将有很大不同。
\subsection{word2vec 的权重和分布式表示}
如前所述，word2vec 中使用的网络有两个权重，分别是输入侧的全连接层的权重（$\bm{W}_{in}$）和输出侧的全连接层的权重（$\bm{W}_{out}$）。一般而言，输入侧的权重 $\bm{W}_{in}$ 的每一行对应于各个单词的分布式表示。另外，输出侧的权重 $\bm{W}_{out}$ 也同样保存了对单词含义进行了解码的向量。只是，输出侧的权重在列方向上保存了各个单词的分布式表示。

那么，我们最终应该使用哪个权重作为单词的分布式表示呢？这里有三个选项：

\begin{description}
    \item[A.] 只使用输入侧的权重
    \item[B.] 只使用输出侧的权重
    \item[C.] 同时使用两个权重
\end{description}

方案 A 和方案 B 只使用其中一个权重。而在采用方案 C 的情况下，根据如何组合这两个权重，存在多种方式，其中一个方式就是简单地将这两个权重相加。

就 word2vec（特别是 skip-gram 模型）而言，最受欢迎的是方案 A。许多研究中也都仅使用输入侧的权重 $\bm{W}_{in}$ 作为最终的单词的分布式表示。遵循这一思路，我们也使用 $\bm{W}_{in}$ 作为单词的分布式表示。