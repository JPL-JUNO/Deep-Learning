\chapter{word2vec\label{Ch03}}
在 \nameref{Ch02} 中，我们使用基于计数的方法得到了单词的分布式表示。本章我们将讨论该方法的替代方法，即基于推理的方法。

\section{基于推理的方法和神经网络}
基于计数的方法使用整个语料库的统计数据（共现矩阵和 PPMI 等），通过一次处理（SVD 等）获得单词的分布式表示。而基于推理的方法使用神经网络，通常在 mini-batch 数据上进行学习。这意味着神经网络一次只需要看一部分学习数据（mini-batch），并反复更新权重。
\subsection{基于推理的方法的概要}
\figures{fig3-2}{基于两边的单词（上下文），预测“？”处出现什么单词}
解开 \autoref{fig3-2} 中的推理问题并学习规律，就是基于推理的方法的主要任
务。通过反复求解这些推理问题，可以学习到单词的出现模式。从“模型视
角”出发，这个推理问题如 \autoref{fig3-3} 所示。

\begin{tcolorbox}
    基于推理的方法和基于计数的方法一样，也基于分布式假设。分布式假设假设“单词含义由其周围的单词构成”。基于推理的方法将这一假设归结为了上面的预测问题。由此可见，不管是哪种方法，如何对基于分布式假设的“单词共现”建模都是最重要的研究主题。
\end{tcolorbox}
\figures{fig3-3}{基于推理的方法：输入上下文，模型输出各个单词的出现概率}
\subsection{神经网络中单词的处理方法}
神经网络无法直接处理 you 或 say 这样的单词，要用神经网络处理单词，需要先将单词转化为固定长度的向量。对此，一种方式是将单词转换为 one-hot 表示（one-hot 向量）。在 one-hot 表示中，只有一个元素是 1，其他元素都是 0。像这样，只要将单词转化为固定长度的向量，神经网络的输入层的神经元个数就可以固定下来。
\section{简单的 word2vec}
\begin{tcolorbox}
    word2vec 一词最初用来指程序或者工具，但是随着该词的流行，在某些语境下，也指神经网络的模型。正确地说，CBOW 模型和 skip-gram 模型是 word2vec 中使用的两个神经网络。
\end{tcolorbox}

\subsection{CBOW模型的推理}
CBOW 模型的输入是上下文。这个上下文用 \verb|['you', 'goodbye']| 这样的单词列表表示。我们将其转换为 one-hot 表示，以便 CBOW 模型可以进行处理。在此基础上，CBOW 模型的网络可以画成 \autoref{fig3-9} 这样。

\autoref{fig3-9} 是 CBOW 模型的网络。它有两个输入层（这里，因为我们对上下文仅考虑两个单词，所以输入层有两个。如果对上下文考虑 $N$ 个单词，则输入层会有 $N$ 个。），经过中间层到达输出层。这里，从输入层到中间层的变换由相同的全连接层（权重为 $\bm{W}_{in}$）完成，从中间层到输出层神经元的变换由另一个全连接层（权重为 $\bm{W}_{out}$）完成。

\figures{fig3-9}{CBOW 模型的网络结构}

中间层的神经元数量比输入层少这一点很重要。中间层需要将预测单词所需的信息压缩保存，从而产生密集的向量表示。这时，中间层被写入了我们人类无法解读的代码，这相当于“编码”工作。而从中间层的信息获得期望结果的过程则称为“解码”。这一过程将被编码的信息复原为我们可以理解的形式。

\subsection{CBOW 模型的学习}
CBOW 模型只是学习语料库中单词的出现模式。如果语料库不一样，学习到的单词的分布式表示也不一样。比如，只使用“体育”相关的文章得到的单词的分布式表示，和只使用“音乐”相关的文章得到的单词的分布式表示将有很大不同。
\subsection{word2vec 的权重和分布式表示}
如前所述，word2vec 中使用的网络有两个权重，分别是输入侧的全连接层的权重（$\bm{W}_{in}$）和输出侧的全连接层的权重（$\bm{W}_{out}$）。一般而言，输入侧的权重 $\bm{W}_{in}$ 的每一行对应于各个单词的分布式表示。另外，输出侧的权重 $\bm{W}_{out}$ 也同样保存了对单词含义进行了解码的向量。只是，输出侧的权重在列方向上保存了各个单词的分布式表示。

那么，我们最终应该使用哪个权重作为单词的分布式表示呢？这里有三个选项：

\begin{description}
    \item[A.] 只使用输入侧的权重
    \item[B.] 只使用输出侧的权重
    \item[C.] 同时使用两个权重
\end{description}

方案 A 和方案 B 只使用其中一个权重。而在采用方案 C 的情况下，根据如何组合这两个权重，存在多种方式，其中一个方式就是简单地将这两个权重相加。

就 word2vec（特别是 skip-gram 模型）而言，最受欢迎的是方案 A。许多研究中也都仅使用输入侧的权重 $\bm{W}_{in}$ 作为最终的单词的分布式表示。遵循这一思路，我们也使用 $\bm{W}_{in}$ 作为单词的分布式表示。
\section{word2vec 的补充说明}
\subsection{CBOW 模型和概率}
下面，我们用数学式来表示当给定上下文 $w_{t-1}$ 和 $w_{t+1}$ 时目标词为 $w_t$ 的概率。使用后验概率，有下式：
\begin{equation}
    \label{eq3-1}
    P(w_t|w_{t-1}, w_{t+1})
\end{equation}

也就是说，CBOW 模型可以建模为 \autoref{eq3-1}。

如果将其扩展到整个语料库，则损失函数可以写为：
\begin{equation}
    L=-\frac{1}{T}\sum_{t=1}^{T}\log P(w_t|w_{t-1}, w_{t+1})
\end{equation}

\subsection{skip-gram 模型}
skip-gram 模型如前所述，word2vec 有两个模型：一个是我们已经讨论过的 CBOW模型；另一个是被称为 skip-gram 的模型。skip-gram 是反转了 CBOW 模型处理的上下文和目标词的模型。

如 \autoref{fig3-23} 所示，CBOW 模型从上下文的多个单词预测中间的单词（目标词），而 skip-gram 模型则从中间的单词（目标词）预测周围的多个单词（上下文）。
\figures{fig3-23}{CBOW 模型和 skip-gram 模型处理的问题}

我们来考虑根据中间单词（目标词）$w_t$ 预测上下文 $w_{t-1}$ 和 $w_{t+1}$ 的情况。此时，skip-gram 可以建模为：
\begin{equation}
    P(w_{t-1}, w_{t+1}|w_t)
\end{equation}
在 skip-gram 模型中，假定上下文的单词之间没有相关性（正确地说是假定“条件独立”）
\begin{equation}
    P(w_{t-1}, w_{t+1}|w_t)=P(w_{t-1}|w_t)P(w_{t+1}|w_t)
\end{equation}

如果扩展到整个语料库，则skip-gram 模型的损失函数可以表示为：
\begin{equation}
    L=-\frac{1}{T}\sum_{t=1}^{T}\log P(w_{t-1}|w_t)+\log P(w_{t+1}|w_t)
\end{equation}
\subsection{基于计数与基于推理}
首先，我们考虑需要向词汇表添加新词并更新单词的分布式表示的场景。此时，基于计数的方法需要从头开始计算。即便是想稍微修改一下单词的分布式表示，也需要重新完成生成共现矩阵、进行 SVD 等一系列操作。相反，基于推理的方法（word2vec）允许参数的增量学习。具体来说，可以将之前学习到的权重作为下一次学习的初始值，在不损失之前学习到的经验的情况下，高效地更新单词的分布式表示。在这方面，基于推理的方法（word2vec）具有优势。

其次，两种方法得到的单词的分布式表示的性质和准确度有什么差异呢？就分布式表示的性质而言，基于计数的方法主要是编码单词的相似性，而 word2vec（特别是 skip-gram 模型）除了单词的相似性以外，还能理解更复杂的单词之间的模式。关于这一点，word2vec 因能解开“$king - man + woman = queen$”这样的类推问题而知名。