\chapter{自然语言和单词的分布式表示\label{Ch02}}
\section{同义词词典}
回顾自然语言处理的历史，人们已经尝试过很多次类似这样的人工定义单词含义的活动。但是，目前被广泛使用的并不是《新华字典》那样的常规词典，而是一种被称为同义词词典（thesaurus）的词典。在同义词词典中，具有相同含义的单词（同义词）或含义类似的单词（近义词）被归类到同一个组中。比如，使用，我们可以知道 car 的同义词有 automobile、motorcar 等
\subsection{WordNet}
在自然语言处理领域，最著名的同义词词典是 WordNet。使用 WordNet，可以获得单词的近义词，或者利用单词网络。使用单词网络，可以计算单词之间的相似度。
\subsection{同义词词典的问题}
\begin{itemize}
    \item 难以顺应时代变化
    \item 人力成本高
    \item 无法表示单词的微妙差异
\end{itemize}
\section{基于计数的方法}
从介绍基于计数的方法开始，我们将使用语料库（corpus）。简而言之，语料库就是大量的文本数据。不过，语料库并不是胡乱收集数据，一般收集的都是用于自然语言处理研究和应用的文本数据。

\begin{tcolorbox}
    自然语言处理领域中使用的语料库有时会给文本数据添加额外的信息。比如，可以给文本数据的各个单词标记词性。在这种情况下，为了方便计算机处理，语料库通常会被结构化（比如，采用树结构等数据形式）。这里，假定我们使用的语料库没有添加标签，而是作为一个大的文本文件，只包含简单的文本数据。
\end{tcolorbox}

能不能将类似于颜色的向量表示方法运用到单词上呢？更准确地说，可否在单词领域构建紧凑合理的向量表示呢？接下来，我们将关注能准确把握单词含义的向量表示。在自然语言处理领域，这称为分布式表示。

单词的分布式表示将单词表示为固定长度的向量。这种向量的特征在于它是用密集向量表示的。密集向量的意思是，向量的各个元素（大多数）是由非0实数表示的。例如，三维分布式表示是 $[0.21,-0.45,0.83]$。
\subsection{分布式假设}
“某个单词的含义由它周围的单词形成”，称为分布式假设（distributional hypothesis）。分布式假设所表达的理念非常简单。单词本身没有含义，单词含义由它所在的上下文（语境）形成。上下文是指某个居中单词的周围词汇。这里，我们将上下文的大小（即周围的单词有多少个）称为\textbf{窗口大小}（window size）。窗口大小为 1，上下文包含左右各 1 个单词；窗口大小为 2，上下文包含左右各 2 个单词，以此类推。
\subsection{共现矩阵}
\figures{fig2-7}{用表格汇总各个单词的上下文中包含的单词的频数}

\subsection{向量间的相似度}
测量向量间的相似度有很多方法，其中具有代表性的方法有向量内积或欧式距离等。虽然除此之外还有很多方法，但是在测量单词的向量表示的相似度方面，\textbf{余弦相似度}（cosine similarity）是很常用的。设有 $\bm{x} = (x_1, x_2, x_3,\cdots , x_n)$ 和 $\bm{y} = (y_1, y_2, y_3,\cdots, y_n)$ 两个向量，它们之间的余弦相似度的定义如下式所示：
\begin{equation}
    similarity(\bm{x},\bm{y})=\frac{x_1y_1+\cdots+x_ny_n}{\sqrt{x_1^2+\cdots+x_n^2}\sqrt{y_1^2+\cdots+y_n^2}}
\end{equation}

余弦相似度直观地表示了“两个向量在多大程度上指向同一方向”。两个向量完全指向相同的方向时，余弦相似度为 1；完全指向相反的方向时，余弦相似度为 -1。
\section{基于计数的方法的改进}
\subsection{点互信息}
共现矩阵的元素表示两个单词同时出现的次数。但是，这种“原始”的次数并不具备好的性质。如果我们看一下高频词汇（出现次数很多的单词），就能明白其原因了。比如，我们来考虑某个语料库中 the 和 car 共现的情况。这意味着，仅仅因为 the 是个常用词，它就被认为与 car 有很强的相关性。为了解决这一问题，可以使用点互信息（Pointwise Mutual Information，PMI）这一指标。对于随机变量 x 和 y，它们的 PMI 定义如下：
\begin{equation}
    PMI(x, y)=\log_2\frac{P(x, y)}{P(x)P(y)}
\end{equation}
其中，$P(x)$ 表示 $x$ 发生的概率，$P(y)$ 表示 $y$ 发生的概率，$P(x, y)$ 表示 $x$ 和 $y$ 同时发生的概率。PMI 的值越高，表明相关性越强。在自然语言的例子中，$P(x)$ 就是指单词 $x$ 在语料库中出现的概率。

虽然我们已经获得了PMI这样一个好的指标，但是 PMI 也有一个问题。那就是当两个单词的共现次数为 0 时，$\log_20 = -\infty$。为了解决这个问题，实践上我们会使用下述正的点互信息（Positive PMI，PPMI）：
\begin{equation}
    \label{ppmi}
    PPMI(x, y)=\max(0, PMI(x, y))
\end{equation}
根据式 \autoref{ppmi}，当 PMI 是负数时，将其视为 0，这样就可以将单词间的相关性表示为大于等于 0 的实数。

但是，这个 PPMI 矩阵还是存在一个很大的问题，那就是随着语料库的词汇量增加，各个单词向量的维数也会增加。而且，其中很多元素都是 0。这表明向量中的绝大多数元素并不重要，也就是说，每个元素拥有的“重要性”很低。另外，这样的向量也容易受到噪声影响，稳健性差。
\subsection{降维}
\begin{tcolorbox}
    向量中的大多数元素为 0 的矩阵（或向量）称为稀疏矩阵（或稀疏向量）。这里的重点是，从稀疏向量中找出重要的轴，用更少的维度对其进行重新表示。结果，稀疏矩阵就会被转化为大多数元素均不为0 的密集矩阵。这个密集矩阵就是我们想要的单词的分布式表示。
\end{tcolorbox}

降维的方法有很多，这里我们使用奇异值分解（Singular Value Decomposition，SVD）。SVD 将任意矩阵分解为 3 个矩阵的乘积，如下式所示：
\begin{equation}
    \bm{X}=\bm{U}\bm{S}\bm{V}^T
\end{equation}
其中 $\bm{U}$ 和 $\bm{V}$ 是列向量彼此正交的正交矩阵，$\bm{S}$ 是除了对角线元素以外其余元素均为 0 的对角矩阵。
\figures{fig2-9}{基于 SVD 的降维示意图（白色部分表示元素为 0）}
\begin{tcolorbox}
    单词的共现矩阵是正方形矩阵，但在 \autoref{fig2-9} 中，为了和之前的图一致，画的是长方形。另外，这里对 SVD 的介绍仅限于最直观的概要性的说明。
\end{tcolorbox}
\section{小结}
使用基于同义词词典的方法，需要人工逐个定义单词之间的相关性。这样的工作非常费力，在表现力上也存在限制（比如，不能表示细微的差别）。而基于计数的方法从语料库中自动提取单词含义，并将其表示为向量。具体来说，首先创建单词的共现矩阵，将其转化为 PPMI 矩阵，再基于 SVD 降维以提高稳健性，最后获得每个单词的分布式表示。另外，我们已经确认过，这样的分布式表示具有在含义或语法上相似的单词在向量空间上位置相近的性质。