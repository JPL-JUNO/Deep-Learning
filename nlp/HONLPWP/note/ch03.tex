\chapter{深度学习和 TensorFlow\label{ch03}}
\section{深度学习}
\subsection{感知器}
当在标记好的训练数据集上训练时，它可以基于输入和输出来学习线性映射。线性映射是一组输入变量（即特征）权重乘积的总和。在处理分类问题时，最终的总和通过一个阶跃函数来选择 0 或者 1。感知器如 \autoref{fig3-1} 所示。
\figures{fig3-1}{感知器模型，这是最简单的神经网络模型。}
\subsection{激活函数}
激活函数是神经网络的重要组成部分，可以将神经网络节点的输入转换为非线性输出，这使得神经网络能够从数据中学习任意非线性映射或模式。
\subsubsection*{sigmoid}
sigmoid 激活函数可以将任何输入转换为概率分布输出。总的来说，sigmoid 函数将任意值压缩或映射到 0 和 1 之间的值，因此被广泛用于二进制分类任务，其输出可被视为属于该类的概率。sigmoid 函数类似于单位阶跃函数，但其曲线更为平滑。这种平滑性确保了函数在整个取值范围内的差异性，而这在网络训练期间是必需的。
\subsubsection*{双曲正切函数}
与 sigmoid 函数相比，双曲线函数在其范围内具有更高的梯度。该激活函数也称 tanh 函数。
\subsubsection*{ReLU}
ReLU 限制了从负值到 0 的输入，但对于正值输入，其输出与输入相同。对于正值而言，ReLU函数具有恒定的梯度，但对于负值而言其梯度为 0。由此可以看出，对于负值输入，ReLU 根本不会触发。该激活函数的计算复杂度低于前述几个函数，因此其预测将更快一些。
\subsection{神经网络}
\subsubsection*{独热编码}
独热编码是用于标记数据（尤其是分类数据）的向量化技术。对于二进制标签而言，目标变量将显示为[0, 1]和[1, 0]，对于三个类别，相同的表示形式将显示为[0, 0, 1]、[0, 1, 0]和[1, 0, 0]。独热编码支持任意数量的类别，其主要优点在于，与任意分类标注方法相比，它对所有类别数据一视同仁。例如，虽然可以使用诸如 0、1 和 2 之类的整数来表示颜色（例如红色、绿色和蓝色）的类别，但尽管颜色本身没有固有顺序，某些 ML 模型却可能会将此类输入视为按序排列的。独热编码避免了这种情况：因为分类值是二进制编码的，所以它不会假定分类值具有任何顺序。
\subsubsection*{softmax}
softmax将任意值的向量归一化或压缩到 0和 1之间的概率分布，其向量输出的总和等同于 1。因此，它通常用于神经网络的最后一层来预测输出类别可能的概率。以下是下标为 $j$ 向量的 softmax函数数学表达式：
\begin{equation}
    Softmax(z_j) = \frac{e^{z_j}}{\sum_{k=1}^{K}e^{z_j}}
\end{equation}
\subsubsection*{交叉熵}
交叉熵是分类任务在训练期间的损失。它实际上计算了 softmax 概率或预测与真实分类之间的差异。以下是二分类问题中交叉熵的表达式，其输出用概率 $\hat{y}$ 表示，真实值用 $y$ 表示：
\begin{equation}
    CrossEntropy = -y\log(\hat{y})-(1-y)\log(1-\hat{y})
\end{equation}
可以看出，当预测的概率接近 1 而真实输出为 0 时，交叉熵将增加或进行惩罚。同样，表达式可以扩展到 $K$ 个类别时的情况。
\subsection{卷积神经网络}
卷积神经网络（CNN）通过学习卷积核来对数据进行变换。
\subsubsection*{核}
核是 CNN 用来在特征图（feature map）上滑动的小窗口。如 \autoref{fig3-10} 所示，核进行滑动窗口运动并生成输出的特征图。核可以选用不同大小的矩形并以较长或较短的步长进行移动。
\figures{fig3-10}{核是 CNN 用来在特征图（feature map）上滑动的小窗口。}
\subsubsection*{最大池化}
最大池化（max pooling）是一种从小窗口中选取最大值的二次抽样形式。
\subsection{递归神经网络}
递归神经网络（RNN）可以训练语言等存在时序依赖的模型。实际上，它可以用于训练任何种类的序列数据。在 RNN 中，神经元的输出会被作为下一时刻的输入反馈到其自身。展开后的 RNN 如 \autoref{fig3-12} 所示。
\figures{fig3-12}{递归神经网络}
由于 RNN 从先前时间步中获取输入的性质，其中很久以前序列中的数据将会丢失。
\subsubsection*{长短期记忆网络}
长短期记忆网络（LSTM）可以通过使用“遗忘门”来记住很久以前的事。相对于 RNN 而言，LSTM 优点在于可以长时间保持记忆。LSTM 中有好几个门，诸如遗忘门、输出门和输入门，每个都有自己的功能，如 \autoref{fig3-13} 所示。
\figures{fig3-13}{长短期记忆网络}