\chapter{Tensors\label{Ch02}}
\section{Creating Tensors}
\begin{table}
    \centering
    \caption{Tensor creation functions}
    \begin{tabularx}{\textwidth}{XX}
        \hline
        Function                                                                                                                                 & Description                                                                                \\
        \hline
        \textsf{torch.tensor(data, dtype=None, device=None, requires\_grad=False, pin\_memory=False)}                                            & Creates a tensor from an existing data structure                                           \\
        \textsf{torch.empty(*size, out=None, dtype=None, layout=torch.strided, device=None, requires\_grad=False)}                               & Creates a tensor from uninitialized elements based on the random state of values in memory \\
        \textsf{torch.zeros(*size, out=None, dtype=None, layout=torch.strided, evice=None, requires\_grad=False)}                                & Creates a tensor with all elements initialized  to 0.0                                     \\
        \textsf{torch.ones(*size, out=None, dtype=None, layout=torch.strided, device=None, requires\_grad=False)}                                & Creates a tensor with all elements initialized to 1.0                                      \\
        \textsf{torch.arange(start=0, end, step=1, out=None, dtype=None, layout=torch.strided, device=None, requires\_grad=False)}               & Creates a 1D tensor of values over a range with a common step value                        \\
        \textsf{torch.linspace(start, end, steps=100, out=None, dtype=None, layout=torch.strided, device=None, requires\_grad=False)}            & Creates a 1D tensor of linearly spaced points between the start and end                    \\
        \textsf{torch.logspace(start, end, steps=100, base=10.0, out=None, dtype=None, layout=torch.strided, device=None, requires\_grad=False)} & Creates a 1D tensor of logarithmically spaced points between the start and end             \\
        \textsf{torch.eye(n, m=None, out=None, dtype=None, layout=torch.strided, device=None, requires\_grad=False)}                             & Creates a 2D tensor with ones on the diagonal and zeros everywhere else                    \\
        \textsf{torch.full(size, fill\_value, out=None, dtype=None, layout=torch.strided, device=None, requires\_grad=False)}                    & Creates a tensor filled with fill\_value                                                   \\
        \textsf{torch.load(f)}                                                                                                                   & Loads a tensor from a serialized pickle file                                               \\
        \textsf{torch.save(f)}                                                                                                                   & Saves a tensor to a serialized pickle file                                                 \\
        \hline
    \end{tabularx}
\end{table}

Use \textsf{torch.arange()} when the step size is known. Use \textsf{torch.linspace()} when the number of elements is known. You can use \textsf{torch.tensor()} to create tensors from array-like structures such as lists, NumPy arrays, tuples, and
sets. To convert existing tensors to NumPy arrays and lists,
use the \textsf{torch.numpy()} and \textsf{torch.tolist()} functions,
respectively.

\subsection{Data Types}
To reduce space complexity, you may sometimes want to
reuse memory and overwrite tensor values using in-place
operations. To perform in-place operations, append the
underscore (\_) postfix to the function name. For example,
the function \textsf{y.add\_(x)} adds x to y, but the results will be
stored in y.
\subsection{Creating Tensors from Random Samples}
\href{www}{Table: Random sampling functions}
\subsection{Creating Tensors Like Other Tensors}
You may want to create and initialize a tensor that has
similar properties to another tensor, including the dtype,
device, and layout properties to facilitate calculations.
Many of the tensor creation operations have a similarity function that allows you to easily do this. The similarity functions
will have the postfix \textsf{\_like}. For example, \textsf{torch.empty\_like(tensor\_a)} will create an empty tensor with the dtype, device, and
layout properties of \textsf{tensor\_a}. Some examples of similarity
functions include \textsf{empty\_like()}, \textsf{zeros\_like()}, \textsf{ones\_like()},
\textsf{full\_like()}, \textsf{rand\_like()}, \textsf{randn\_like()}, and \textsf{rand\_int\_like()}.
\section{Tensor Operations}
\subsection{Indexing, Slicing, Combining, and Splitting Tensors}
The following key distinctions and best practices are important to keep in mind:
\begin{itemize}
    \item \textsf{item()} is an important and commonly used function to return the Python number from a tensor containing a single value.
    \item Use \textsf{view()} instead of reshape() for reshaping tensors in most cases. Using reshape() may cause the tensor to be copied, depending on its layout in memory. view() ensures that it will not be copied.
    \item Using x.T or x.t() is a simple way to transpose 1D or 2D tensors. Use transpose() when dealing with multidimensional tensors.
    \item The \textsf{torch.squeeze()} function is used often in deep learning to remove an unused dimension. For example, a batch of images with a single image can be reduced from 4D to 3D using squeeze().
    \item The \textsf{torch.unsqueeze()} function is often used in deep learning to add a dimension of size 1. Since most PyTorch models expect a batch of data as an input, you could apply unsqueeze() when you only have one data sample. For example, you can pass a 3D image into torch.unsqueeze() to create a batch of one image.
\end{itemize}
\href{www}{Table: Indexing, slicing, combining, and splitting operations} lists some commonly used functions to manipulate
tensor elements.

\subsection{Tensor Operations for Mathematics}
\href{wwww}{Table: Pointwise operations list some commonly used pointwise operations}
Three different syntaxes can be used for most tensor operations. Tensors support operator overloading, so you can
use operators directly, as in \textsf{z = x + y}. Although you can
also use PyTorch functions such as torch.add() to do the
same thing, this is less common. Lastly, you can perform
in-place operations using the underscore (\_) postfix. The
function \textsf{y.add\_(x)} achieves the same results, but theyâ€™ll
be stored in y.

Comparison functions seem pretty straightforward; however, there are a few key points to keep in mind. Common pitfalls
include the following:
\begin{itemize}
    \item The torch.eq() function or == returns a tensor of the same size with a Boolean result for each element. The torch.equal() function tests if the tensors are the same size, and if all elements within the tensor are equal then it returns a single Boolean value.
    \item The function torch.allclose() also returns a single Boolean value if all elements are close to a specified value.
\end{itemize}
\href{www}{Tabel: Spectral and other math operations} lists some built-in operations for spectrum analysis and other mathematical operations.